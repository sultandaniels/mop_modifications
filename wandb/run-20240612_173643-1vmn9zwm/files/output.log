batch_size: 28
train_steps: 7
context length: 250
num_tasks: 4
output_dir: ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C
checkpoint_callback /Users/sultandaniels/Documents/Transformer_Kalman/outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints
ckpt_path: ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints/step=7.ckpt
Checkpoint file ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints/step=7.ckpt does not exist.
Epoch 0:   0%|                                                                                                                                                                                             | 0/7 [00:00<?, ?it/s]
GPU available: True (mps), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  | Name      | Type      | Params
----------------------------------------
0 | _read_in  | Linear    | 768
1 | _backbone | GPT2Model | 9.1 M
2 | _read_out | Linear    | 645
----------------------------------------
9.1 M     Trainable params
0         Non-trainable params
9.1 M     Total params
36.304    Total estimated model params size (MB)
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.





Epoch 0: 100%|â–ˆ| 7/7 [00:15<00:00,  0.45it/s, v_num=9zwm, train_loss_mse_step=0.290, train_metric_mse_ts0_dim_0_step=0.216, train_metric_mse_ts0_dim_1_step=0.304, train_metric_mse_ts0_dim_2_step=0.0609, train_metric_mse_ts0_d
`Trainer.fit` stopped: `max_epochs=1` reached.
ckpt_path ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints/step=7.ckpt
config path: ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints/step=7.ckpt
Number of validation systems: 3
Number of traces: 2000
parent_dir: ../outputs/GPT2/240612_173642.6bf05c_unifA_unif_C/checkpoints
