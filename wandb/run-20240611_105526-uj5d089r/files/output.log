GPU available: True (mps), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  | Name      | Type      | Params
----------------------------------------
0 | _read_in  | Linear    | 768
1 | _backbone | GPT2Model | 9.1 M
2 | _read_out | Linear    | 645
----------------------------------------
9.1 M     Trainable params
0         Non-trainable params
9.1 M     Total params
36.304    Total estimated model params size (MB)
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
batch_size: 28
train_steps: 7
context length: 250
num_tasks: 40
output_dir: ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C
checkpoint_callback /Users/sultandaniels/Documents/Transformer_Kalman/outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints
ckpt_path: ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints/step=7.ckpt
Checkpoint file ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints/step=7.ckpt does not exist.
/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.




Epoch 0: 100%|█| 7/7 [00:09<00:00,  0.74it/s, v_num=089r, train_loss_mse_step=0.150, train_metric_mse_ts0_dim_0_step=0

Epoch 0: 100%|█| 7/7 [00:11<00:00,  0.62it/s, v_num=089r, train_loss_mse_step=0.150, train_metric_mse_ts0_dim_0_step=0
ckpt_path ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints/step=7.ckpt
config path: ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints/step=7.ckpt
Number of validation systems: 3
Number of traces: 2000
parent_dir: ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C/checkpoints
parent_parent_dir: ../outputs/GPT2/240611_105520.ca288c_gaussA_gauss_C
Traceback (most recent call last):
  File "/Users/sultandaniels/Documents/Transformer_Kalman/mop_modifications/streamlined_mop/src/data_train.py", line 104, in <module>
    create_plots(config, run_preds, run_deg_kf_test, excess, num_systems=config.num_val_tasks, shade=shade)
  File "/Users/sultandaniels/Documents/Transformer_Kalman/mop_modifications/streamlined_mop/src/create_plots_with_zero_pred.py", line 573, in create_plots
    save_preds(run_deg_kf_test, config) #save the predictions to a file
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/Documents/Transformer_Kalman/mop_modifications/streamlined_mop/src/create_plots_with_zero_pred.py", line 462, in save_preds
    err_lss, irreducible_error = compute_errors(config, config.C_dist, run_deg_kf_test, wentinn_data=False)  #, emb_dim)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/Documents/Transformer_Kalman/mop_modifications/streamlined_mop/src/create_plots_with_zero_pred.py", line 262, in compute_errors
    _, flattened_preds_tf = model.predict_step({"xs": torch.from_numpy(flattened_I).to(device)}) #.float().to(device)})    # predict using the model
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/Documents/Transformer_Kalman/mop_modifications/streamlined_mop/src/models/gpt2.py", line 35, in predict_step
    output = self._backbone(inputs_embeds=embeds).last_hidden_state
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 888, in forward
    outputs = block(
              ^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 390, in forward
    attn_outputs = self.attn(
                   ^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 331, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sultandaniels/anaconda3/envs/mop/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 183, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt