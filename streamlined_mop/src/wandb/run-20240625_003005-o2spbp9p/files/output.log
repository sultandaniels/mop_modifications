batch_size: 28
train_steps: 40000
context length: 250
num_tasks: 40000
output_dir: ../outputs/GPT2/240625_002549.1e49ad_upperTriA_gauss_C
checkpoint_callback /home/sultand/mop_modifications/streamlined_mop/outputs/GPT2/240625_002549.1e49ad_upperTriA_gauss_C/checkpoints
ckpt_path: ../outputs/GPT2/240625_002549.1e49ad_upperTriA_gauss_C/checkpoints/step=40000.ckpt
Checkpoint file ../outputs/GPT2/240625_002549.1e49ad_upperTriA_gauss_C/checkpoints/step=40000.ckpt does not exist.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/sultand/.conda/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
/home/sultand/.conda/envs/mop/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name      | Type      | Params | Mode
------------------------------------------------
0 | _read_in  | Linear    | 768    | train
1 | _backbone | GPT2Model | 9.1 M  | train
2 | _read_out | Linear    | 645    | train
------------------------------------------------
9.1 M     Trainable params
0         Non-trainable params
9.1 M     Total params
36.304    Total estimated model params size (MB)
/home/sultand/.conda/envs/mop/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=254` in the `DataLoader` to improve performance.








































































































































































































































































































































































































































































































































































































































































































































































































































































